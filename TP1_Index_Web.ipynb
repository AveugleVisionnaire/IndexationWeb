{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Mini Projet Indexation Web "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np \n",
    "import re #For regular expressions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import os \n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pickle \n",
    "import gzip\n",
    "import operator \n",
    "from multiprocessing import Pool\n",
    "import glob\n",
    "from functools import reduce\n",
    "import warnings\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "lem = WordNetLemmatizer()\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and cleaning data (texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, get the list of paths to all data files and then read them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_rep=glob.glob(\"data/**/*.txt\",recursive=True)\n",
    "\n",
    "def readText(path):\n",
    "'''\n",
    "Takes a path and returns the corresponding file as python string\n",
    "'''\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paralellized loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14348483085632324\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "with Pool(8) as pool:\n",
    "    list_text = pool.map(readText, list_rep)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading with for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058347463607788086"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading data in a list\n",
    "start=time.time()\n",
    "list_text = []\n",
    "for path in list_rep:\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "        list_text.append(text)\n",
    "end=time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "1. Tokenization\n",
    "    - Lower text case\n",
    "    - Remove stopwords\n",
    "    -Apply lemmatization \n",
    "2. Building the vocabulary: words and frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanText(text, stopwords=stops, lem = lem):\n",
    "    \n",
    "    \"\"\"Take a text file and put it in lower case\n",
    "    then remove stopwords,\n",
    "    and apply stemming to each word \n",
    "    \"\"\"\n",
    "    text = re.findall(\"[a-zA-z0-9]+\", text.lower())\n",
    "    return [lem.lemmatize(elt) for elt in text if elt not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.764699935913086"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "with Pool() as pool:\n",
    "    list_processed_texts = pool.map(cleanText, list_text)\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.967571258544922"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "list_processed_texts = list(map(cleanText, list_text))\n",
    "end = time.time()\n",
    "end-start\n",
    "####Slower than map with pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocabOneFile(termlist):\n",
    "    \"\"\"\n",
    "        This function takes a list of words and return a dictionary\n",
    "        with frequency for each word\n",
    "    \"\"\"\n",
    "    return Counter(termlist)\n",
    "\n",
    "def vocabGlobal(termlists):\n",
    "    \"\"\"\n",
    "    This function is the generalization of vocabOneFile which takes a list of words and return a dictionary\n",
    "        with frequency for each word. So it returns global vocabulary(words,frequencies) of a list of documents.\n",
    "    \"\"\"\n",
    "    myCounter = Counter()\n",
    "    for text in termlists:\n",
    "        myCounter += Counter(text)\n",
    "    return dict(myCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.24228000640869"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "with Pool(8) as pool:\n",
    "    vocabulary = pool.map(vocabOneFile, list_processed_texts)\n",
    "vocabulary = dict(reduce(lambda x,y:x+y, vocabulary))\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1786749362945557"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "global_vocabulary = vocabGlobal(list_processed_texts)\n",
    "end = time.time()\n",
    "end-start\n",
    "####faster than map with pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def index_one_file(termlist):\n",
    "    \"\"\"\n",
    "    This function take a list of terms and a dictionnary containing word as key and their positions as value.\n",
    "    input : doc = [word1, word2, ...]\n",
    "    output = {word1: [pos1, pos2], word2: [pos2, pos434], ...}\n",
    "    \"\"\"\n",
    "    fileIndex = defaultdict(list)\n",
    "    for index, word in enumerate(termlist):\n",
    "        fileIndex[word].append(index)\n",
    "    return dict(fileIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comptime(func,args,n): #args is the list of arguments for func in the order\n",
    "    t=0\n",
    "    s=time.time()\n",
    "    for i in range(n): \n",
    "        tmp=func(*args)\n",
    "    e=time.time()\n",
    "    return (e-s)/n # Faster than the first implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8066937923431396"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#With map\n",
    "s=time.time()\n",
    "with Pool(8) as pool:\n",
    "    dic1=pool.map(index_one_file,list_processed_texts)\n",
    "dic1={k:dic1[k] for k in range(len(dic1))}\n",
    "e=time.time()\n",
    "e-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4753296375274658"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Index with dictionary comprehension : two times Faster than mapReduce\n",
    "e=time.time()\n",
    "index={k:index_one_file(list_processed_texts[k]) for k in range(len(list_processed_texts))}\n",
    "s=time.time()\n",
    "s-e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0003': [217],\n",
       " '0840': [165],\n",
       " '10': [4],\n",
       " '110': [40],\n",
       " '13': [65],\n",
       " '14': [9],\n",
       " '15': [66, 102],\n",
       " '150': [90],\n",
       " '160': [91],\n",
       " '1995': [47],\n",
       " '2': [103, 176, 183, 215],\n",
       " '2423': [216],\n",
       " '26': [162, 168],\n",
       " '3': [128],\n",
       " '38': [127],\n",
       " '4': [114],\n",
       " '42': [214],\n",
       " '5': [5, 10, 110],\n",
       " '6': [113],\n",
       " '7': [41, 50],\n",
       " '80': [184],\n",
       " '87': [177],\n",
       " '9': [111],\n",
       " '939': [163],\n",
       " '945': [169],\n",
       " '95': [49],\n",
       " 'accounting': [126],\n",
       " 'added': [118, 211],\n",
       " 'allowing': [141],\n",
       " 'also': [147],\n",
       " 'analyst': [22, 61, 132],\n",
       " 'august': [12, 107, 117],\n",
       " 'average': [63],\n",
       " 'bank': [180],\n",
       " 'basket': [175],\n",
       " 'bear': [191],\n",
       " 'benefit': [206],\n",
       " 'better': [198],\n",
       " 'billion': [6, 11, 42, 51, 67, 92],\n",
       " 'buoyed': [13],\n",
       " 'bureau': [27],\n",
       " 'central': [179],\n",
       " 'certainly': [209],\n",
       " 'competitive': [143],\n",
       " 'continued': [125],\n",
       " 'crown': [7, 43, 52, 68, 93, 146, 160, 205],\n",
       " 'csu': [28, 97],\n",
       " 'currency': [174],\n",
       " 'czech': [1, 25],\n",
       " 'data': [29],\n",
       " 'dealer': [203],\n",
       " 'deficit': [3, 48, 58, 94, 196],\n",
       " 'deviation': [172],\n",
       " 'digested': [158],\n",
       " 'dipping': [150],\n",
       " 'dollar': [164],\n",
       " 'earlier': [31],\n",
       " 'economist': [86],\n",
       " 'encouraged': [134],\n",
       " 'end': [96],\n",
       " 'equipment': [123],\n",
       " 'estimate': [192],\n",
       " 'export': [108, 145],\n",
       " 'fall': [0],\n",
       " 'far': [44],\n",
       " 'figure': [74, 153, 188],\n",
       " 'finance': [88],\n",
       " 'fixing': [182],\n",
       " 'forecast': [62, 82, 89],\n",
       " 'forex': [202],\n",
       " 'full': [46, 80],\n",
       " 'gap': [64],\n",
       " 'getting': [197],\n",
       " 'gmt': [166],\n",
       " 'good': [16, 76, 144],\n",
       " 'growth': [120],\n",
       " 'happily': [69],\n",
       " 'high': [39],\n",
       " 'hit': [37],\n",
       " 'implies': [171],\n",
       " 'import': [100, 124, 131, 137],\n",
       " 'imported': [17],\n",
       " 'increase': [116],\n",
       " 'industrial': [19],\n",
       " 'industry': [140],\n",
       " 'january': [33],\n",
       " 'kupka': [85],\n",
       " 'local': [201],\n",
       " 'loss': [156],\n",
       " 'machinery': [121],\n",
       " 'make': [77],\n",
       " 'market': [14, 157, 186],\n",
       " 'martin': [84],\n",
       " 'midpoint': [173],\n",
       " 'month': [57],\n",
       " 'much': [207],\n",
       " 'needed': [138],\n",
       " 'newsroom': [213],\n",
       " 'number': [208],\n",
       " 'one': [56, 200],\n",
       " 'opening': [167],\n",
       " 'optimistic': [73],\n",
       " 'patria': [87],\n",
       " 'percent': [104, 112, 115, 129, 178, 185],\n",
       " 'pleased': [187],\n",
       " 'positively': [149],\n",
       " 'prague': [212],\n",
       " 'produce': [142],\n",
       " 'quickly': [154],\n",
       " 'rate': [170],\n",
       " 'reacted': [148],\n",
       " 'regaining': [155],\n",
       " 'relatively': [72],\n",
       " 'release': [152],\n",
       " 'released': [30],\n",
       " 'restructure': [139],\n",
       " 'restructuring': [20],\n",
       " 'revise': [79],\n",
       " 'rise': [135],\n",
       " 'rose': [101, 109],\n",
       " 'said': [23, 83, 98, 133, 199],\n",
       " 'sector': [136],\n",
       " 'seem': [190],\n",
       " 'sentiment': [15],\n",
       " 'september': [8, 34, 53, 99],\n",
       " 'shortfall': [36, 54],\n",
       " 'show': [18],\n",
       " 'showed': [32],\n",
       " 'size': [195],\n",
       " 'slightly': [151],\n",
       " 'slowly': [193],\n",
       " 'smallest': [55],\n",
       " 'statistic': [159],\n",
       " 'statistical': [26],\n",
       " 'strong': [119],\n",
       " 'structure': [194],\n",
       " 'sure': [189],\n",
       " 'surpassing': [45],\n",
       " 'surprised': [60, 70],\n",
       " 'think': [71, 204],\n",
       " 'though': [75],\n",
       " 'thursday': [24],\n",
       " 'time': [38],\n",
       " 'total': [130],\n",
       " 'track': [21],\n",
       " 'trade': [2, 35],\n",
       " 'trading': [161],\n",
       " 'transport': [122],\n",
       " 'u': [78],\n",
       " 'weaken': [210],\n",
       " 'wednesday': [181],\n",
       " 'year': [59, 81, 95, 105, 106]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0003': [217],\n",
       " '0840': [165],\n",
       " '10': [4],\n",
       " '110': [40],\n",
       " '13': [65],\n",
       " '14': [9],\n",
       " '15': [66, 102],\n",
       " '150': [90],\n",
       " '160': [91],\n",
       " '1995': [47],\n",
       " '2': [103, 176, 183, 215],\n",
       " '2423': [216],\n",
       " '26': [162, 168],\n",
       " '3': [128],\n",
       " '38': [127],\n",
       " '4': [114],\n",
       " '42': [214],\n",
       " '5': [5, 10, 110],\n",
       " '6': [113],\n",
       " '7': [41, 50],\n",
       " '80': [184],\n",
       " '87': [177],\n",
       " '9': [111],\n",
       " '939': [163],\n",
       " '945': [169],\n",
       " '95': [49],\n",
       " 'accounting': [126],\n",
       " 'added': [118, 211],\n",
       " 'allowing': [141],\n",
       " 'also': [147],\n",
       " 'analyst': [22, 61, 132],\n",
       " 'august': [12, 107, 117],\n",
       " 'average': [63],\n",
       " 'bank': [180],\n",
       " 'basket': [175],\n",
       " 'bear': [191],\n",
       " 'benefit': [206],\n",
       " 'better': [198],\n",
       " 'billion': [6, 11, 42, 51, 67, 92],\n",
       " 'buoyed': [13],\n",
       " 'bureau': [27],\n",
       " 'central': [179],\n",
       " 'certainly': [209],\n",
       " 'competitive': [143],\n",
       " 'continued': [125],\n",
       " 'crown': [7, 43, 52, 68, 93, 146, 160, 205],\n",
       " 'csu': [28, 97],\n",
       " 'currency': [174],\n",
       " 'czech': [1, 25],\n",
       " 'data': [29],\n",
       " 'dealer': [203],\n",
       " 'deficit': [3, 48, 58, 94, 196],\n",
       " 'deviation': [172],\n",
       " 'digested': [158],\n",
       " 'dipping': [150],\n",
       " 'dollar': [164],\n",
       " 'earlier': [31],\n",
       " 'economist': [86],\n",
       " 'encouraged': [134],\n",
       " 'end': [96],\n",
       " 'equipment': [123],\n",
       " 'estimate': [192],\n",
       " 'export': [108, 145],\n",
       " 'fall': [0],\n",
       " 'far': [44],\n",
       " 'figure': [74, 153, 188],\n",
       " 'finance': [88],\n",
       " 'fixing': [182],\n",
       " 'forecast': [62, 82, 89],\n",
       " 'forex': [202],\n",
       " 'full': [46, 80],\n",
       " 'gap': [64],\n",
       " 'getting': [197],\n",
       " 'gmt': [166],\n",
       " 'good': [16, 76, 144],\n",
       " 'growth': [120],\n",
       " 'happily': [69],\n",
       " 'high': [39],\n",
       " 'hit': [37],\n",
       " 'implies': [171],\n",
       " 'import': [100, 124, 131, 137],\n",
       " 'imported': [17],\n",
       " 'increase': [116],\n",
       " 'industrial': [19],\n",
       " 'industry': [140],\n",
       " 'january': [33],\n",
       " 'kupka': [85],\n",
       " 'local': [201],\n",
       " 'loss': [156],\n",
       " 'machinery': [121],\n",
       " 'make': [77],\n",
       " 'market': [14, 157, 186],\n",
       " 'martin': [84],\n",
       " 'midpoint': [173],\n",
       " 'month': [57],\n",
       " 'much': [207],\n",
       " 'needed': [138],\n",
       " 'newsroom': [213],\n",
       " 'number': [208],\n",
       " 'one': [56, 200],\n",
       " 'opening': [167],\n",
       " 'optimistic': [73],\n",
       " 'patria': [87],\n",
       " 'percent': [104, 112, 115, 129, 178, 185],\n",
       " 'pleased': [187],\n",
       " 'positively': [149],\n",
       " 'prague': [212],\n",
       " 'produce': [142],\n",
       " 'quickly': [154],\n",
       " 'rate': [170],\n",
       " 'reacted': [148],\n",
       " 'regaining': [155],\n",
       " 'relatively': [72],\n",
       " 'release': [152],\n",
       " 'released': [30],\n",
       " 'restructure': [139],\n",
       " 'restructuring': [20],\n",
       " 'revise': [79],\n",
       " 'rise': [135],\n",
       " 'rose': [101, 109],\n",
       " 'said': [23, 83, 98, 133, 199],\n",
       " 'sector': [136],\n",
       " 'seem': [190],\n",
       " 'sentiment': [15],\n",
       " 'september': [8, 34, 53, 99],\n",
       " 'shortfall': [36, 54],\n",
       " 'show': [18],\n",
       " 'showed': [32],\n",
       " 'size': [195],\n",
       " 'slightly': [151],\n",
       " 'slowly': [193],\n",
       " 'smallest': [55],\n",
       " 'statistic': [159],\n",
       " 'statistical': [26],\n",
       " 'strong': [119],\n",
       " 'structure': [194],\n",
       " 'sure': [189],\n",
       " 'surpassing': [45],\n",
       " 'surprised': [60, 70],\n",
       " 'think': [71, 204],\n",
       " 'though': [75],\n",
       " 'thursday': [24],\n",
       " 'time': [38],\n",
       " 'total': [130],\n",
       " 'track': [21],\n",
       " 'trade': [2, 35],\n",
       " 'trading': [161],\n",
       " 'transport': [122],\n",
       " 'u': [78],\n",
       " 'weaken': [210],\n",
       " 'wednesday': [181],\n",
       " 'year': [59, 81, 95, 105, 106]}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building the Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First implementation\n",
    "Create inverted index from list of list_processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_inv_index(data):\n",
    "    index = defaultdict(list)\n",
    "    res={}\n",
    "    \n",
    "    for i, words in enumerate(data):\n",
    "        for word in words:\n",
    "            index[word].append(i)\n",
    "    index_new = {}\n",
    "    for word in index.keys():\n",
    "        index_new[word] = dict( Counter( index[word] ) )\n",
    "        for doc_num in index_new[word].keys():\n",
    "            pos = np.array((np.where(np.array(data[doc_num])==word))).tolist()\n",
    "            index_new[word][doc_num] = {\"occurencies\":index_new[word][doc_num], \"positions\":pos[0], \"doc_size\":len(data[doc_num])}\n",
    "        res[word] = index_new[word]\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.024723291397095"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "test_Index = create_inv_index(list_processed_texts)\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second implementation\n",
    "Create inverted index from the normal index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverted_index(index):\n",
    "    \"\"\"\n",
    "    This function takes a normal index and return the inverted index. More details below:\n",
    "    input = {doc_id: {word: [pos1, pos2, ...], ... }}\n",
    "    output = {word1: {doc_id:{doc_size,[pos1, pos2],freq},....}, ..., ...}\n",
    "    \"\"\"\n",
    "    inv_index = defaultdict(dict)\n",
    "    for doc_id in index.keys():\n",
    "        doc_size=sum([len(list_pos) for list_pos in index[doc_id].values()]) #computes doc_size from list of positions in indexe\n",
    "        for word in index[doc_id].keys():\n",
    "\n",
    "            inv_index[word][doc_id]={'doc_size':doc_size,\n",
    "                              'positions':index[doc_id][word],\n",
    "                              'occurencies':len(index[doc_id][word])}\n",
    "    return dict(inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8610539674758911"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=0\n",
    "s=time.time()\n",
    "for i in range(10): \n",
    "    inv_index=inverted_index(index)\n",
    "e=time.time()\n",
    "(e-s)/10 # Faster than the first implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "inverted_index() takes 1 positional argument but 2500 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-65f04dda1ff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-984a310e1252>\u001b[0m in \u001b[0;36mcomptime\u001b[0;34m(func, args, n)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m \u001b[0;31m# Faster than the first implementation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: inverted_index() takes 1 positional argument but 2500 were given"
     ]
    }
   ],
   "source": [
    "comptime(inverted_index,index,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inv_index[\"china\"]\n",
    "# *[1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize inverted index with *pickle* and *gzip*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store the object\n",
    "with gzip.open(\"Index_Articles.pklz\", \"wb\") as fout:\n",
    "    pickle.dump(inv_index,fout)\n",
    "    \n",
    "\n",
    "#restore the object\n",
    "with gzip.open(\"Index_Articles.pklz\", \"rb\") as fin:\n",
    "    indexe_pkl= pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indexe_pkl[\"china\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Search functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-word Queries - Function *sing_woq(word,index)*\n",
    "In which documents does a given word occur?\n",
    "To do that, we check if the word is in inverted index keys and return the corresponding documents otherwise the function returns None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sing_woq(query, index):\n",
    "    \"\"\"\n",
    "    This function takes a query, tokenizes it and then checks if the word is in the\n",
    "    inverted index keys and return the corresponding documents;\n",
    "    otherwise the function returns None \n",
    "    \"\"\"\n",
    "    query = cleanText(query)\n",
    "    if len(query)==1 and query[0] in index.keys(): ### ensure the query is a single word and exists in the vocab\n",
    "        return list(index[query[0]].keys())\n",
    "    elif len(query)>1 : ##The user provided more than one word\n",
    "        warnings.warn(\"Please provide a single word!\")\n",
    "        return None\n",
    "    else : return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 16, 17, 22, 25, 26, 29, 37, 42, 846, 924, 1005, 1020, 1930, 1945]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1 = sing_woq(\"tournament\", indexe_pkl)\n",
    "query1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sing_woq(\"the\", indexe_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free-text Queries\n",
    "* Which documents contain at least one word from a given list of words?\n",
    "\n",
    "We split the query in a list of words and then apply the previous function to each word.\n",
    "The returned object is the union of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def free_tq(query, index):\n",
    "    \n",
    "    \"\"\"\n",
    "        This function takes free text as query and return a list of all docs containing at least one term\n",
    "        from the query\n",
    "    \"\"\"\n",
    "    \n",
    "    list_words=query.split()\n",
    "    res = [id_doc for word in list_words if sing_woq(word, index) for id_doc in sing_woq(word, index) ]\n",
    "    if len(res)>0:\n",
    "        return sorted(list(set(res)))\n",
    "    else:\n",
    "        print(\"No match found!!!\")\n",
    "        return None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 16, 17, 22, 25, 26, 29, 37, 42]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_tq(\"The british tournament \", inv_index)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001119852066040039"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comptime(free_tq,[\"the british\",inv_index],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Queries\n",
    "* Which documents contain a given phrase in the same order?\n",
    "\n",
    "\n",
    "1. We're looking for the texts belonging to the intersection of sing_woq for every word in the query text.\n",
    "2. Then we check whether they are in correct order or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phrase_query(query, index):\n",
    "    \"\"\"\n",
    "        This function takes a phrase and return documents containing all the phrase's terms in the same order.\n",
    "        1. We're looking for the texts belonging to the intersection of sing_woq for every word in the query text.\n",
    "        2. Then we check whether they are in correct order or not.\n",
    "    \"\"\"\n",
    "    list_words = cleanText(query) #cleaning query\n",
    "    res = set()\n",
    "    final_result = []\n",
    "    \n",
    "    #### Retrieve the list of docs containing all the terms in the query\n",
    "    l=[set(sing_woq(word,index)) for word in list_words] #we use set because list doesn't recognize intersection as method\n",
    "    ids=list(set.intersection(*l)) #gets id of documents containing all words in the query\n",
    "    \n",
    "    \n",
    "    #Check whether terms are in correct order\n",
    "    if len(ids) > 0: # If there is at least one document containing all words in the query\n",
    "        \n",
    "        # getting positions of words in documents\n",
    "        for id_doc in ids: \n",
    "            posMinusOne = []\n",
    "            list_pos=[index[word][id_doc][\"positions\"] for word in list_words] \n",
    "            for i, lpos in enumerate(list_pos) : #Check whether terms are in correct order\n",
    "                posMinusOne.append( set([x-i for x in lpos]) )\n",
    "                \n",
    "            if (len(set.intersection(*posMinusOne)) > 0):\n",
    "                final_result.append(id_doc)\n",
    "                \n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text[533]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[533]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query3 = phrase_query(\"who will head the enlarged group.\\nAdvance Bank shareholders would be offered a combination of A$2.10 in cash, a 20 cent special cash dividend and new St George shares up to a value of A$5.00\", inv_index)\n",
    "query3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordering Queries  according to different metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordering single word queries by term frequency in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sing_woq_TF(query,ind):\n",
    "    \"\"\"\n",
    "        This fucntion takes a single word as query and return all the documents containing the query \n",
    "        ordered by the term's frequency in theses docs\n",
    "    \"\"\"\n",
    "    \n",
    "    query = cleanText(query) ##cleaning the query\n",
    "    if len(query)==1 and query[0] in ind.keys():\n",
    "        word=query[0]\n",
    "        d={doc:ind[word][doc][\"occurencies\"] for doc in ind[word].keys()} # A dict containing the document id as key and the frequency of the word in the document\n",
    "        \n",
    "        sorted_d = sorted(d.items(), key=operator.itemgetter(1),reverse=True) #Sorted list of tuples (doc_ids,frequency) in descending order\n",
    "        return sorted_d\n",
    "    \n",
    "    elif len(query)>1: ##The user provided more than one word\n",
    "        warnings.warn(\"Please provide a single word !!!\")\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        print(\"Word not found !!!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00023764848709106445"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comptime(sing_woq_TF,[\"china\",inv_index],100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordering single word queries by weighted term frequency (normalized by the doc size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sing_woq_TF_weighted(ind,query):\n",
    "    \"\"\"\n",
    "        This fucntion takes a single word as query and return all the documents containing the query \n",
    "        ordered by the term's normalized frequency in theses docs\n",
    "    \"\"\"\n",
    "    query = cleanText(query)\n",
    "    if len(query)==1 and query[0] in ind.keys():\n",
    "        word=query[0]\n",
    "        d={doc:ind[word][doc][\"occurencies\"]/ind[word][doc][\"doc_size\"] for doc in ind[word].keys()} # A dict containing the document id as key and the frequency of the word in the document\n",
    "        import operator \n",
    "        sorted_d = sorted(d.items(), key=operator.itemgetter(1),reverse=True) #Sorted list of tuples (doc_ids,frequency) in descending order\n",
    "        return sorted_d\n",
    "    \n",
    "    elif len(query)>1: ##The user provided more than one word\n",
    "        warnings.warn(\"Please provide a single word !!!\")\n",
    "        return None\n",
    "\n",
    "    else:\n",
    "        print(\"Word not found !!!\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1025, 0.036312849162011177),\n",
       " (1002, 0.03571428571428571),\n",
       " (1030, 0.033112582781456956),\n",
       " (1040, 0.028735632183908046),\n",
       " (1022, 0.02702702702702703),\n",
       " (1044, 0.023872679045092837),\n",
       " (1434, 0.023255813953488372),\n",
       " (1033, 0.022813688212927757),\n",
       " (1014, 0.022653721682847898),\n",
       " (1429, 0.022284122562674095)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sing_woq_TF_weighted(inv_index,\"french\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.036312849162011177, 0.03571428571428571, 0.033112582781456956)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((list_processed_texts[1025].count(\"french\")/len(list_processed_texts[1025])),\n",
    "(list_processed_texts[1002].count(\"french\")/len(list_processed_texts[1002])),\n",
    "((list_processed_texts[1030].count(\"french\")/len(list_processed_texts[1030]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordered queries weighted by tf_idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sing_wq_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "        This fucntion takes a single word as query and return all the documents containing the query \n",
    "        ordered by the term's frequency normalized by term_freq*inverse_doc_freq  in theses docs\n",
    "    \"\"\"\n",
    "    query = cleanText(query)\n",
    "    \n",
    "    if len(query)==1 and len(list(index[query[0]].keys())): ##Check if the query is contained in the index\n",
    "        tmp = list(index[query[0]].items())\n",
    "        N = len(tmp) ### Number of documents in which the query appears\n",
    "        D = len(index) ##size of our index\n",
    "        tmp = sorted( list(map(lambda x:(x[0], \n",
    "                                         (x[1][\"occurencies\"]/x[1][\"doc_size\"])*np.log(D/(1+N)),\n",
    "                                        ), tmp)),\n",
    "                     key=lambda a:a[1], reverse=True)\n",
    "        return list(tmp)\n",
    "    \n",
    "    elif len(query)>1: ##The user provided more than one word\n",
    "        warnings.warn(\"Please provide a single word !!!\")\n",
    "        return None\n",
    "\n",
    "    else:\n",
    "        print(\"Word not found !!!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(427, 0.092865052631146416),\n",
       " (756, 0.071893983647055223),\n",
       " (1785, 0.070586820308017856),\n",
       " (1153, 0.069274797625712689),\n",
       " (1166, 0.067911518080600267),\n",
       " (421, 0.067193223177824696),\n",
       " (404, 0.065309301032652034),\n",
       " (657, 0.065157064899708794),\n",
       " (401, 0.063993545883642561),\n",
       " (158, 0.063818221100399711)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sing_wq_tf_idf(\"economy\", inv_index)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Model - VSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=sorted(inv_index.keys()) #get words from texts in alphabetic order \n",
    "len(sorted(inv_index.keys()))>len(global_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2=sorted(global_vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in corpus if word not in corpus2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_sparse(doc_id=None,text=None):\n",
    "    \"\"\"\n",
    "    This function take either a id of document or query(text) and return its a list of 3 elements [data, row, col] \n",
    "    needed for creating its sparse representation:\n",
    "    data: sparse matric none null elements ie frequencies\n",
    "    col: corresponding column for element in data\n",
    "    row: corresponding row for element in data\n",
    "    \n",
    "    \"\"\"\n",
    "    data = []; row = []; col = []\n",
    "    \n",
    "    if doc_id and not text:\n",
    "        for word in index[doc_id].keys():\n",
    "            j=corpus.index(word)\n",
    "            data.append(len(index[doc_id][word]))\n",
    "            row.append(doc_id)\n",
    "            col.append(j)\n",
    "        return [data, row, col]\n",
    "    if text and not doc_id and len(cleanText(text))>0:\n",
    "        text=cleanText(text)\n",
    "        for word in text:\n",
    "            if word in corpus:\n",
    "                j=corpus.index(word)\n",
    "                data.append(text.count(word))\n",
    "                col.append(j)\n",
    "                row.append(0)   \n",
    "        return [data, row, col]\n",
    "    if (doc_id and text):\n",
    "        warnings.warn(\"This function requires one argument\")\n",
    "        return None\n",
    "    if not doc_id and not text:\n",
    "        warnings.warn(\"This function requires one argument\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_sparse_doc(doc_id):\n",
    "    \"\"\"\n",
    "    A version of pre_sparse() for document. Needed in map and reduce\n",
    "    \"\"\"\n",
    "    data = []; row = []; col = []\n",
    "    for word in index[doc_id].keys():\n",
    "        j=corpus.index(word)\n",
    "        data.append(len(index[doc_id][word]))\n",
    "        row.append(doc_id)\n",
    "        col.append(j)\n",
    "    return [data, row, col]\n",
    "\n",
    "\n",
    "\n",
    "def pre_sparse_text(text):\n",
    "    \"\"\"\n",
    "    A version of pre_sparse() text. Needed in map and reduce\n",
    "    \"\"\"\n",
    "    data = []; row = []; col = []\n",
    "    \n",
    "    if text and len(cleanText(text))>0:\n",
    "        text=cleanText(text)\n",
    "        for word in text:\n",
    "            if word in corpus:\n",
    "                j=corpus.index(word)\n",
    "                data.append(text.count(word))\n",
    "                col.append(j)\n",
    "                row.append(0)   \n",
    "        return [data, row, col]\n",
    "    else:\n",
    "        warnings.warn(\"Review your text\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [0], [4345]]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csr_matrix(([1,2],([0,0],[2,3]))).todense()\n",
    "pre_sparse_doc(doc_id=245)\n",
    "pre_sparse_text(\"I'm british\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparse(text=None,nrow=len(list_rep),ncol=len(corpus)): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the sparse reprenstation of the its argument text. If text not provided it returns the \n",
    "    sparse matrix all documents. \n",
    "    \"\"\"\n",
    "    \n",
    "    #if thext is empty the function return sparse matrix for all documents and the corpus\n",
    "    #nrow=0 if sparsing vector else number of documents\n",
    "    if text:\n",
    "        pre_sps=pre_sparse_text(text=text)\n",
    "        res=csr_matrix((pre_sps[0], (pre_sps[1], pre_sps[2])),shape=(1,ncol))\n",
    "        return res, normalize(res,norm=\"l1\",axis=1) #normalize is from sklearn\n",
    "    \n",
    "    else:\n",
    "        with Pool(8) as pool:\n",
    "             data_and_pos= pool.map(pre_sparse_doc,list(range(nrow)))\n",
    "        add = lambda x,y:x+y\n",
    "        data= reduce(add,[item[0] for item in data_and_pos],[])\n",
    "        rows= reduce(add,[item[1] for item in data_and_pos],[])\n",
    "        cols= reduce(add,[item[2] for item in data_and_pos],[])\n",
    "        res= csr_matrix((data, (rows, cols)),shape=(nrow,ncol))\n",
    "        return res, normalize(res,norm=\"l1\",axis=1) #normalize is from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.6634566783905"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1  =time.time()\n",
    "spm,spm_normalized=sparse()\n",
    "e1 = time.time()\n",
    "e1-s1\n",
    "# list(zip(list(range(21)),repeat(None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 26058)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16: {'doc_size': 282, 'occurencies': 1, 'positions': [180]},\n",
       " 17: {'doc_size': 222, 'occurencies': 2, 'positions': [125, 170]},\n",
       " 38: {'doc_size': 174, 'occurencies': 1, 'positions': [126]},\n",
       " 42: {'doc_size': 292, 'occurencies': 1, 'positions': [194]},\n",
       " 83: {'doc_size': 357, 'occurencies': 1, 'positions': [345]},\n",
       " 98: {'doc_size': 364, 'occurencies': 1, 'positions': [354]},\n",
       " 106: {'doc_size': 247, 'occurencies': 1, 'positions': [27]},\n",
       " 110: {'doc_size': 242, 'occurencies': 1, 'positions': [121]},\n",
       " 144: {'doc_size': 215, 'occurencies': 1, 'positions': [124]},\n",
       " 146: {'doc_size': 213, 'occurencies': 1, 'positions': [122]}}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_index[\"american\"]\n",
    "dict(list(inv_index[\"american\"].items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eutsche Morgan Grenfell of any managers found to bear responsibility for failing to spot irregular dealings by former fund manager Peter Young is expected next week, banking s'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_text[55][25:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0090090090090090089, 2)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_normalized[17,corpus.index(\"american\")],spm[17,corpus.index(\"american\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.04761905,  0.04761905,  0.04761905,  0.04761905,  0.04761905,\n",
       "          0.04761905,  0.04761905,  0.04761905,  0.19047619,  0.04761905,\n",
       "          0.04761905,  0.04761905]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spmvec=sparse(text=list_text[55][25:200])[1]\n",
    "spmvec.todense()[0,[corpus.index(word) for word in list_text[55][25:200].split() if word in corpus]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.todense()[140,corpus.index(\"french\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_normalized.todense()[0,:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize the sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store the object\n",
    "with gzip.open(\"spm.pklz\", \"wb\") as fout:\n",
    "    pickle.dump(spm,fout)\n",
    "with gzip.open(\"spm_normalized.pklz\", \"wb\") as fout:\n",
    "    pickle.dump(spm_normalized,fout)\n",
    "    \n",
    "\n",
    "#restore the object\n",
    "with gzip.open(\"spm_normalized.pklz\", \"rb\") as fin:\n",
    "    spm_norm_pkl= pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_norm_pkl.todense()[0,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot product ordering documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(436, array([ 0.00835623])),\n",
       " (55, array([ 0.00824774])),\n",
       " (113, array([ 0.00771159])),\n",
       " (444, array([ 0.00613497])),\n",
       " (268, array([ 0.0058072])),\n",
       " (431, array([ 0.00568586])),\n",
       " (446, array([ 0.00551572])),\n",
       " (432, array([ 0.00532387])),\n",
       " (768, array([ 0.0052381])),\n",
       " (770, array([ 0.0052381]))]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spmvec=sparse(text=list_text[55][25:200])[1]\n",
    "dotp=spm_normalized.dot(spmvec.transpose()).toarray()\n",
    "d={k:dotp[k] for k in range(dotp.shape[0])}\n",
    "sorted_d = sorted(d.items(), key=operator.itemgetter(1),reverse=True)\n",
    "sorted_d[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eutsche Morgan Grenfell of any managers found to bear responsibility for failing to spot irregular dealings by former fund manager Peter Young is expected next week, banking s'"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_text[55][25:200]\n",
    "# lem.lemmatize(\"Management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hong Kong funds are expected to erect \"Chinese walls\" between asset management and traders after revelations of unsanctioned trades at the colony\\'s biggest fund manager, Jardine Fleming Investment Management.\\nCentral dealing, a system that prevents fund managers from executing their own trades, is gaining favour in Asia after last week\\'s shocking disclosure of late allocation of trades by one of Hong Kong\\'s most prominent fund managers, Colin Armstrong.\\n\"There is an awareness in the pension (fund) community that this is an issue in Hong Kong,\" said Gregory Neumann, executive director at Scudder Stevens &amp; Clark Asia Ltd. \"I think this could turn out to be a positive for the investment management industry -- no-one will get hired going forward without central dealing.\"\\nRegulators unveiled severe punishment last week for Armstrong\\'s actions, which involved delaying the allocation of his trades until the price had changed. Some of the deals involved his own personal trading account.\\nLondon\\'s Investment Management Regulatory Organisation (IMRO) and the Securities and Futures Commission (SFC) in Hong Kong fined Jardine Fleming 700,000 stg, revoked the registration for Jardine Fleming Investment Management\\'s former chief executive and cancelled the authorisation for Jardine Fleming Asset Management in London.\\nJardine Fleming, jointly owned by the Jardine Matheson and Robert Fleming groups, also paid US$19.3 million in compensation.\\nRegulators said they were convinced Jardine Fleming was committed to establishing a culture of regulatory compliance, and that central dealing was one useful step towards that goal.\\n\"Central dealing simply means all the people managing money should place their orders through a central desk,\" said Deborah Glass, senior director at the SFC.\\n\"If a fund manager can run his orders through all the brokers in town it\\'s very difficult to track.\"\\nPlacing another layer between the fund manager and his or her client creates a Chinese wall, reducing the opportunity for a fund manager to manipulate his or her orders. Three people are involved the transaction rather than two.\\nChinese walls are supposed to be invisible but inviolate divisions between asset management operations that invest for clients, and sister broking and corporate finance departments whose clients are often bought by the funds.\\nJardine Fleming said it would have central dealing desks set up in Tokyo and Hong Kong by the end of the year.\\nMost major investment managers in New York and London employ central dealing, but a quick review of major fund houses in Hong Kong revealed that many here do not.\\nOne of Jardine Fleming\\'s major competitors said all trades were put through different trading desks dedicated to individual markets rather than a central desk. Another competitor said a new automated order input system would eliniate the need for a central desk.\\nOther fund managers warned that central desks do not eradicate the opportunity for fraud, especially if there is a culture of non-compliance among the organisation\\'s dealers.\\nIt would be possible for fund managers and brokers on a central dealing desk to work together to move markets in a favourable direction before certain trades are executed.\\n\"From time to time people in Hong Kong have had trouble with dealers,\" the fund manager warned. \"It removes one area but it opens up the door to another.\"\\nRegulators last week stressed the importance of a \"culture\" of compliance throughout an organisation, and said they would be carefully watching Jardine Fleming and other fund companies operating in Hong Kong.\\n\"One of the leading asset consulting firms here who was, I believe, acting on behalf of several institutional pension clients, just called to reconfirm that the managers of those various pension clients did have central dealing,\" Neumann said.\\n\"This raises concerns for individuals in any institution,\" said Esther Heathcote, vice-president at BT Fund Management. \"They have to have regard for internal controls and compliance.\"\\nAs a subsidiary of an American bank based in Australia, BT Fund Management is regulated by the Australian Securities Commission and the U.S. Securities and Exchange Commission.\\n'"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_text[436]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def free_tqV2(text,spm=spm_normalized,n=10):\n",
    "    \"\"\"\n",
    "    This function takes a text and return the n most correlated documents with it. \n",
    "    output: [(doc1, corr1),(doc2,corr2),()...]\n",
    "    \"\"\"\n",
    "    spmvec=sparse(text)[1]\n",
    "    \n",
    "    dotp=spm_normalized.dot(spmvec.transpose()).toarray()\n",
    "    d={k:dotp[k] for k in range(dotp.shape[0])}\n",
    "    sorted_d=sorted(d.items(), key=operator.itemgetter(1),reverse=True)[:n]\n",
    "    return [(item[0],item[1][0]) for item in sorted_d] # just to simplify the display \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(436, 0.008356227106227106),\n",
       " (55, 0.008247743541861188),\n",
       " (113, 0.0077115866589550783),\n",
       " (444, 0.0061349693251533744),\n",
       " (268, 0.0058072009291521487),\n",
       " (431, 0.0056858564321250887),\n",
       " (446, 0.0055157198014340872),\n",
       " (432, 0.0053238686779059439),\n",
       " (768, 0.005238095238095237),\n",
       " (770, 0.005238095238095237)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_tqV2(list_text[55][25:200])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
