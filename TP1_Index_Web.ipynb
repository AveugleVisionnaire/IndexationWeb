{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEXATION WEB : Python rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import gzip\n",
    "from multiprocessing import Pool\n",
    "import glob\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue; text-align:center\">1. TEXT DATA LOADING AND CLEANING</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\">First, get the list of paths to all data files and then read them</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_rep=glob.glob(\"data/**/*.txt\",recursive=True)\n",
    "def readText(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# with Pool(2) as pool:\n",
    "#     list_text = pool.map(readText, list_rep)\n",
    "# end = time.time()\n",
    "# print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading data in a list\n",
    "start=time.time()\n",
    "list_text = []\n",
    "for path in list_rep:\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "        list_text.append(text)\n",
    "end=time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\">Pre-processing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanText(text, stopwords=stops, lem = lem):\n",
    "    \n",
    "    \"\"\"Take a text file and put it in lower case\n",
    "    then remove stopwords,\n",
    "    and apply stemming to each word \n",
    "    \"\"\"\n",
    "    text = re.findall(\"[a-zA-z0-9]+\", text.lower())\n",
    "    return [lem.lemmatize(elt) for elt in text if elt not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with Pool(8) as pool:\n",
    "    list_processed_texts = pool.map(cleanText, list_text)\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_processed_texts[62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "list_processed_texts = list(map(cleanText, list_text))\n",
    "end = time.time()\n",
    "end-start\n",
    "####Slower than map with pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_processed_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##Récupérer La liste des mots de mon vocabulaire\n",
    "# list_vocab = list(myCounter.keys())\n",
    "# len(list_vocab)\n",
    "len(list_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\">Building the Vocabulary </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocabOneFile(termlist):\n",
    "    \"\"\"\n",
    "        this function takes a list of words and return a dictionary\n",
    "        with frequency for each word\n",
    "    \"\"\"\n",
    "    return Counter(termlist)\n",
    "\n",
    "def vocabGlobal(my_list):\n",
    "    myCounter = Counter()\n",
    "    for text in my_list:\n",
    "        myCounter += Counter(text)\n",
    "    return dict(myCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with Pool(7) as pool:\n",
    "    vocabulary = pool.map(vocabOneFile, list_processed_texts)\n",
    "vocabulary = dict(reduce(lambda x,y:x+y, vocabulary))\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "global_vocabulary = vocabGlobal(list_processed_texts)\n",
    "end = time.time()\n",
    "end-start\n",
    "####faster than map with pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####Vocabulary size\n",
    "len(vocabulary), len(global_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the indexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input = [word1, word2, ...]\n",
    "#output = {word1: [pos1, pos2], word2: [pos2, pos434], ...}\n",
    "def index_one_file(termlist):\n",
    "    fileIndex = defaultdict(list)\n",
    "    for index, word in enumerate(termlist):\n",
    "        fileIndex[word].append(index)\n",
    "    return dict(fileIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#With map\n",
    "def joindic(x,y): x.update(y); return x\n",
    "s=time.time()\n",
    "with Pool(8) as pool:\n",
    "    dic=pool.map(index_one_file,list_processed_texts)\n",
    "dic=reduce(joindic,dic[1:],dic[0])\n",
    "e=time.time()\n",
    "e-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e=time.time()\n",
    "dic={k:index_one_file(list_processed_texts[k]) for k in range(len(list_processed_texts))}\n",
    "s=time.time()\n",
    "s-e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(dic.keys())\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(word) for word in dic[0].values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_processed_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The fall in the Czech trade deficit to 10.5 billion crowns in September from 14.5 billion in August buoyed market sentiment, and the goods imported show industrial restructuring is on track, analysts said on Thursday.\\nCzech Statistical Bureau (CSU) data released earlier showed the January-September trade shortfall hit an all-time high of 110.7 billion crowns, far surpassing the full 1995 deficit of 95.7 billion crowns.\\nBut the September shortfall, the smallest one-month deficit this year, surprised analysts, who had forecast on average a gap of 13 to 15 billion crowns.\\n\"I\\'m happily surprised. I think it\\'s a relatively optimistic figure, though not so good as to make us revise our full year forecast,\" said Martin Kupka, an economist at Patria Finance, which has forecast a 150-160 billion crown deficit at year-end.\\nThe CSU said September imports rose 15.2 percent year-on-year, the same as in August, while exports rose 5.9 percent after a 6.4 percent increase in August.\\nIt added that strong growth in machinery and transport equipment imports continued, accounting for 38.3 percent of total imports.\\nAnalysts said they were encouraged by the rise in this sector as these imports are needed to restructure industry, allowing it to produce more competitive goods for export.\\nThe crown also reacted positively, dipping slightly on the release of the figures but quickly regaining its losses as the market digested the statistics.\\nThe crown was trading at 26.939 to the dollar at 0840 GMT after opening at 26.945. The rate implies a deviation from the midpoint of the currency basket of +2.87 percent, after the central bank\\'s Wednesday fixing at +2.80 percent.\\n\"The market is pleased with the figures for sure, they seem to bear out estimates that slowly, the structure and size of the deficit is getting better,\" said one local forex dealer.\\n\" don\\'t think the crown itself will benefit too much from the numbers, but it certainly should not weaken,\" he added.\\n-- Prague Newsroom, 42-2-2423-0003\\n'"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building the Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input = {doc_id: {word: [pos1, pos2, ...], ... }}\n",
    "#res = {word1: {doc_id:{doc_size,[pos1, pos2],freq},....}, ..., ...}\n",
    "def inverted_index(index):\n",
    "    inv_index = defaultdict(dict)\n",
    "    for doc_id in index.keys():\n",
    "        for word in index[doc_id].keys():\n",
    "            inv_index[word][doc_id]={'doc_size':len(index[doc_id]),\n",
    "                              'positions':index[doc_id][word],\n",
    "                              'occurencies':len(index[doc_id][word])}\n",
    "    return inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t=0\n",
    "s=time.time()\n",
    "for i in range(10): \n",
    "    inv_index=inverted_index(dic)\n",
    "e=time.time()\n",
    "(e-s)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inv_index[\"china\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_processed_texts[62].count(\"china\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Optimize index creation time\n",
    "def create_index(data):\n",
    "    index = defaultdict(list)\n",
    "    res={}\n",
    "    \n",
    "    for i, words in enumerate(data):\n",
    "        for word in words:\n",
    "            index[word].append(i)\n",
    "    index_new = {}\n",
    "    for word in index.keys():\n",
    "        index_new[word] = dict( Counter( index[word] ) )\n",
    "        for doc_num in index_new[word].keys():\n",
    "            pos = np.array((np.where(np.array(data[doc_num])==word))).tolist()\n",
    "            index_new[word][doc_num] = {\"occurencies\":index_new[word][doc_num], \"positions\":pos[0], \"doc_size\":len(data[doc_num])}\n",
    "        res[word] = index_new[word]\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testind = create_index(clean_Docs)\n",
    "#testind[\"china\"]\n",
    "#dict(Counter([1,2,2,2,3,3,2,5,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#newIndex[\"china\"]\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "test_Index = create_index(list_processed_texts)\n",
    "end = time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize data with gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store the object\n",
    "with gzip.open(\"Index_Articles.pklz\", \"wb\") as fout:\n",
    "    pickle.dump(inv_index,fout)\n",
    "    \n",
    "\n",
    "#restore the object\n",
    "with gzip.open(\"Index_Articles.pklz\", \"rb\") as fin:\n",
    "    indexe_pkl= pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indexe_pkl[\"china\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Search function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quering\n",
    "## Single-word Queries - Function *sing_woq(ind,word)*\n",
    "In which documents does a given word occur?\n",
    "To do that, we loop through the item in the word's information and catch all values for tag *id_doc*. These values append the returned list wich is initialized to an empty list. If the word doesn't exist in the indexe, we return []."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sing_woq(query, index):\n",
    "    query = cleanText(query)\n",
    "    #l=list(index[query[0]].keys())\n",
    "    if len(query)>0 and len(list(index[query[0]].keys())):\n",
    "        return list(index[query[0]].keys())\n",
    "    else:\n",
    "        #print(\"No result for specified key!!!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query1 = sing_woq(\"tournament\", indexe_pkl)\n",
    "query1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sing_woq(\"the\", indexe_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free-text Queries\n",
    "* Which documents contain at least one word from a given list of words?\n",
    "\n",
    "We use the previous founction on the list of words containing the text cleaned from stopwords. The returned object is the union of several results of the previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def free_tq(query, index):\n",
    "    list_words=query.split()\n",
    "    res = [id_doc for word in list_words if sing_woq(word, index) for id_doc in sing_woq(word, index) ]\n",
    "    if len(res)>0:\n",
    "        return sorted(list(set(res)))\n",
    "    else:\n",
    "        print(\"No match found!!!\")\n",
    "        return None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "free_tq(\"tournament \", inv_index)==sing_woq(\"tournament\",inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "free_tq(\"The british tournament \", inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPosition(word, doc, index):\n",
    "    try:\n",
    "        return index[word][doc][\"positions\"]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(getPosition(\"china\", 57, newIndex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Queries\n",
    "* Which documents contain a given phrase in the same order?\n",
    "\n",
    "\n",
    "1. We're looking for the texts belonging to the intersection of sing_woq for every word in the query text.\n",
    "2. The we check whether they are in correct order or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phrase_query(query, index):\n",
    "    list_words = cleanText(query) #cleaning query\n",
    "    res = set()\n",
    "    final_result = []\n",
    "\n",
    "    l=[set(sing_woq(word,index)) for word in list_words]\n",
    "    ids=list(set.intersection(*l)) \n",
    "        \n",
    "    #we use set because list does't recognize intersection as method\n",
    "    #ids=list(set.instersection(*l)) #gets id of documents containing all words in the query\n",
    "    \n",
    "    \n",
    "    #Check whether terms are in correct order\n",
    "    if len(ids) > 0: # If there is at least one document conatins all words in the query\n",
    "        \n",
    "        for id_doc in ids: # getting positions of words in documents\n",
    "            posMinusOne = []\n",
    "            list_pos=[index[word][id_doc][\"positions\"] for word in list_words] \n",
    "#             for word in list_words:\n",
    "#                 list_pos=[item[id_doc][\"positions\"] for item in index[word]] \n",
    "            for i, lpos in enumerate(list_pos) : #Check whether terms are in correct order\n",
    "                posMinusOne.append( set([x-i for x in lpos]) )\n",
    "                \n",
    "            if (len(set.intersection(*posMinusOne)) > 0):\n",
    "                final_result.append(id_doc)\n",
    "                \n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query3 = phrase_query(list_text[1832], inv_index)\n",
    "query3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[inv_index[word][0][\"positions\"] for word in [\"september\",\"fall\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_text[1832]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_index[\"china\"][97]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Autres façon de compter les fréquences des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def search_v3(query, index):\n",
    "    item_list = re.findall(\"[A-Za-z0-9]+\", query)\n",
    "    item_list2 = list(map(str.lower, item_list))\n",
    "    res = set()\n",
    "    final_Result = []\n",
    "    for i in range(len(item_list2)):\n",
    "        tmp = set(search_v1(item_list2[i], index))\n",
    "        if i ==0:\n",
    "            res = tmp.intersection(tmp)\n",
    "        else :\n",
    "            res = res.intersection(tmp)\n",
    "    \n",
    "    if len(res) > 0:\n",
    "        list_pos = []\n",
    "        isNear = []\n",
    "        for doc_num in res:\n",
    "            for item in item_list2:\n",
    "                list_pos.append(getPosition(item, doc_num, index))\n",
    "            min_occurence = min ([len(i) for i in list_pos])\n",
    "            firstDocPos = list_pos[0]\n",
    "            list_other_Docs = list_pos[1:]\n",
    "            for position in firstDocPos:\n",
    "                for other_Pos in list_other_Docs:\n",
    "                    for p in other_Pos:\n",
    "                        isNear.append(abs(p-position))\n",
    "            if (1 in isNear):\n",
    "                final_Result.append(doc_num)\n",
    "    return final_Result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">Ordered Queries</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orderd queries by absolute frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_v1_sorted(query, index):\n",
    "    query = query.lower()\n",
    "    \n",
    "    try :\n",
    "        tmp = list(index[query].items())\n",
    "        tmp = sorted( list(map(lambda x:(x[0], x[1][\"occurencies\"]), tmp)), key=lambda a:a[1], reverse=True)\n",
    "        return list(map(lambda x:x[0], tmp))\n",
    "    \n",
    "    except :\n",
    "        print(\"No result for specified key!!!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_v1_sorted(\"china\", newIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "liste_rep[1407], liste_rep[774], liste_rep[2342]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered queries by weighted frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def search_v1_weighted(query, index):\n",
    "    query = query.lower()\n",
    "    \n",
    "    try :\n",
    "        tmp = list(index[query].items())\n",
    "        tmp = sorted( list(map(lambda x:(x[0], x[1][\"occurencies\"]/x[1][\"doc_size\"]), tmp)),\n",
    "                     key=lambda a:a[1], reverse=True)\n",
    "        return list(map(lambda x:x[0], tmp))\n",
    "    \n",
    "    except :\n",
    "        print(\"No result for specified key!!!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_v1_weighted(\"china\", newIndex)\n",
    "#newIndex[\"china\"][871]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "liste_rep[2409], liste_rep[2449], liste_rep[871]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ordered queries weighted by tf_idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_v1_tf_idf(query, index):\n",
    "    query = query.lower()\n",
    "    \n",
    "    try :\n",
    "        tmp = list(index[query].items())\n",
    "        N = len(tmp)\n",
    "        D = len(index)\n",
    "        tmp = sorted( list(map(lambda x:(x[0], \n",
    "                                         (x[1][\"occurencies\"]/x[1][\"doc_size\"])*np.log(D/(1+N)),\n",
    "                                        ), tmp)),\n",
    "                     key=lambda a:a[1], reverse=True)\n",
    "        return list(map(lambda x:x[0], tmp))\n",
    "    \n",
    "    except :\n",
    "        print(\"No result for specified key!!!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_v1_tf_idf(\"economy\", newIndex) == search_v1_weighted(\"economy\", newIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; text-align:center\">Ordering for free Text queries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_v2_sorted(query, index):\n",
    "    #item_list = re.findall(\"[a-z0-9]+\", query)\n",
    "    item_list = list(map(str.lower, re.findall(\"[a-z0-9]+\", query)))\n",
    "    res = []\n",
    "    for item in item_list:\n",
    "        tmp = search_v1(item, index)\n",
    "        if tmp:\n",
    "            res.extend(tmp)\n",
    "    return list(set(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#newIndex\n",
    "#len(newIndex)\n",
    "len(list_vocab)\n",
    "#clean_Docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Bag of Words</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import Counter, OrderedDict\n",
    "Corpus = list_vocab\n",
    "v = DictVectorizer()\n",
    "\n",
    "v.fit([OrderedDict.fromkeys(Corpus, 1)])\n",
    "X = v.transform(Counter(f) for f in (clean_Docs))\n",
    "\n",
    "#print(type(X))\n",
    "#print(X.A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.where(X.A[0]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "docs = [[\"hello\", \"world\", \"hello\", \"alka\"], [\"goodbye\", \"cruel\", \"world\"]]\n",
    "indptr = [0]\n",
    "indices = []\n",
    "data = []\n",
    "vocabulary = {}\n",
    "for d in docs:\n",
    "    for term in d:\n",
    "        index = vocabulary.setdefault(term, len(vocabulary))\n",
    "        indices.append(index)\n",
    "        data.append(1)\n",
    "    indptr.append(len(indices))\n",
    "\n",
    "#csr_matrix((data, indices, indptr), dtype=int).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary, data, indptr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
