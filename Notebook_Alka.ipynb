{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEXATION WEB : Python rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des modules"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": null,
>>>>>>> dd2c70e64b034f1f834869895664a0412d80115e
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import gzip\n",
    "from multiprocessing import Pool\n",
    "import glob\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue; text-align:center\">1. TEXT DATA LOADING AND CLEANING</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\">First, get the list of paths to all data files and then read them</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_rep=glob.glob(\"data/**/*.txt\",recursive=True)\n",
    "def readText(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# with Pool(2) as pool:\n",
    "#     list_text = pool.map(readText, list_rep)\n",
    "# end = time.time()\n",
    "# print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data in a list\n",
    "start=time.time()\n",
    "list_text = []\n",
    "for path in list_rep:\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "        list_text.append(text)\n",
    "end=time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\">Pre-processing</h3>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2a9b50b5d531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mliste_dirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mliste_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliste_dirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2a9b50b5d531>\u001b[0m in \u001b[0;36mgetFiles\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
>>>>>>> dd2c70e64b034f1f834869895664a0412d80115e
   "source": [
    "def cleanText(text, stopwords=stops, lem = lem):\n",
    "    \n",
    "    \"\"\"Take a text file and put it in lower case\n",
    "    then remove stopwords,\n",
    "    and apply stemming to each word \n",
    "    \"\"\"\n",
    "    text = re.findall(\"[a-zA-z0-9]+\", text.lower())\n",
    "    return [lem.lemmatize(elt) for elt in text if elt not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with Pool(8) as pool:\n",
    "    list_processed_texts = pool.map(cleanText, list_text)\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_processed_texts[62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "list_processed_texts = list(map(cleanText, list_text))\n",
    "end = time.time()\n",
    "end-start\n",
    "####Slower than map with pool"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'liste_rep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-efc8c5a11677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#LOading data in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mliste_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mliste_rep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'liste_rep' is not defined"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> dd2c70e64b034f1f834869895664a0412d80115e
   "source": [
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_processed_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Récupérer La liste des mots de mon vocabulaire\n",
    "# list_vocab = list(myCounter.keys())\n",
    "# len(list_vocab)\n",
    "len(list_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\">Building the Vocabulary </h3>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> dd2c70e64b034f1f834869895664a0412d80115e
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocabOneFile(termlist):\n",
    "    \"\"\"\n",
    "        this function takes a list of words and return a dictionary\n",
    "        with frequency for each word\n",
    "    \"\"\"\n",
    "    return Counter(termlist)\n",
    "\n",
    "def vocabGlobal(my_list):\n",
    "    myCounter = Counter()\n",
    "    for text in my_list:\n",
    "        myCounter += Counter(text)\n",
    "    return dict(myCounter)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'liste_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-43a8c022f675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyCounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobalCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliste_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'liste_text' is not defined"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> dd2c70e64b034f1f834869895664a0412d80115e
   "source": [
    "start = time.time()\n",
    "with Pool(7) as pool:\n",
    "    vocabulary = pool.map(vocabOneFile, list_processed_texts)\n",
    "vocabulary = dict(reduce(lambda x,y:x+y, vocabulary))\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "global_vocabulary = vocabGlobal(list_processed_texts)\n",
    "end = time.time()\n",
    "end-start\n",
    "####faster than map with pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Vocabulary size\n",
    "len(vocabulary), len(global_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the indexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input = [word1, word2, ...]\n",
    "#output = {word1: [pos1, pos2], word2: [pos2, pos434], ...}\n",
    "def index_one_file(termlist):\n",
    "    fileIndex = defaultdict(list)\n",
    "    for index, word in enumerate(termlist):\n",
    "        fileIndex[word].append(index)\n",
    "    return dict(fileIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With map\n",
    "def joindic(x,y): x.update(y); return x\n",
    "s=time.time()\n",
    "with Pool(8) as pool:\n",
    "    dic=pool.map(index_one_file,list_processed_texts)\n",
    "dic=reduce(joindic,dic[1:],dic[0])\n",
    "e=time.time()\n",
    "e-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=time.time()\n",
    "dic={k:index_one_file(list_processed_texts[k]) for k in range(len(list_processed_texts))}\n",
    "s=time.time()\n",
    "s-e"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": null,
>>>>>>> dd2c70e64b034f1f834869895664a0412d80115e
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(dic.keys())\n",
    "dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building the Inverted Index"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'liste_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c92e3b306499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_Docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcleanText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliste_text\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'liste_text' is not defined"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
>>>>>>> dd2c70e64b034f1f834869895664a0412d80115e
   "source": [
    "#input = {doc_id: {word: [pos1, pos2, ...], ... }}\n",
    "#res = {word1: {doc_id:{doc_size,[pos1, pos2],freq},....}, ..., ...}\n",
    "def inverted_index(index):\n",
    "    inv_index = defaultdict(dict)\n",
    "    for doc_id in index.keys():\n",
    "        for word in index[doc_id].keys():\n",
    "            inv_index[word][doc_id]={'doc_size':len(index[doc_id]),\n",
    "                              'positions':index[doc_id][word],\n",
    "                              'occurencies':len(index[doc_id][word])}\n",
    "    return inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "s=time.time()\n",
    "for i in range(10): \n",
    "    inv_index=inverted_index(dic)\n",
    "e=time.time()\n",
    "(e-s)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inv_index[\"china\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_processed_texts[62].count(\"china\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Optimize index creation time\n",
    "def create_index(data):\n",
    "    index = defaultdict(list)\n",
    "    res={}\n",
    "    \n",
    "    for i, words in enumerate(data):\n",
    "        for word in words:\n",
    "            index[word].append(i)\n",
    "    index_new = {}\n",
    "    for word in index.keys():\n",
    "        index_new[word] = dict( Counter( index[word] ) )\n",
    "        for doc_num in index_new[word].keys():\n",
    "            pos = np.array((np.where(np.array(data[doc_num])==word))).tolist()\n",
    "            index_new[word][doc_num] = {\"occurencies\":index_new[word][doc_num], \"positions\":pos[0], \"doc_size\":len(data[doc_num])}\n",
    "        res[word] = index_new[word]\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testind = create_index(clean_Docs)\n",
    "#testind[\"china\"]\n",
    "#dict(Counter([1,2,2,2,3,3,2,5,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#newIndex[\"china\"]\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "test_Index = create_index(list_processed_texts)\n",
    "end = time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize data with gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the object\n",
    "with gzip.open(\"Index_Articles.pklz\", \"wb\") as fout:\n",
    "    pickle.dump(inv_index,fout)\n",
    "    \n",
    "\n",
    "#restore the object\n",
    "with gzip.open(\"Index_Articles.pklz\", \"rb\") as fin:\n",
    "    indexe_pkl= pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indexe_pkl[\"china\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Search function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quering\n",
    "## Single-word Queries - Function *sing_woq(ind,word)*\n",
    "In which documents does a given word occur?\n",
    "To do that, we loop through the item in the word's information and catch all values for tag *id_doc*. These values append the returned list wich is initialized to an empty list. If the word doesn't exist in the indexe, we return []."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sing_woq(query, index):\n",
    "    query = cleanText(query)\n",
    "    #l=list(index[query[0]].keys())\n",
    "    if len(query)>0 and len(list(index[query[0]].keys())):\n",
    "        return list(index[query[0]].keys())\n",
    "    else:\n",
    "        #print(\"No result for specified key!!!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = sing_woq(\"tournament\", indexe_pkl)\n",
    "query1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_woq(\"the\", indexe_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free-text Queries\n",
    "* Which documents contain at least one word from a given list of words?\n",
    "\n",
    "We use the previous founction on the list of words containing the text cleaned from stopwords. The returned object is the union of several results of the previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def free_tq(query, index):\n",
    "    list_words=query.split()\n",
    "    res = [id_doc for word in list_words if sing_woq(word, index) for id_doc in sing_woq(word, index) ]\n",
    "    if len(res)>0:\n",
    "        return sorted(list(set(res)))\n",
    "    else:\n",
    "        print(\"No match found!!!\")\n",
    "        return None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_tq(\"tournament \", inv_index)==sing_woq(\"tournament\",inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_tq(\"The british tournament \", inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPosition(word, doc, index):\n",
    "    try:\n",
    "        return index[word][doc][\"positions\"]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getPosition(\"china\", 57, newIndex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Queries\n",
    "* Which documents contain a given phrase in the same order?\n",
    "\n",
    "\n",
    "1. We're looking for the texts belonging to the intersection of sing_woq for every word in the query text.\n",
    "2. The we check whether they are in correct order or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_query(query, index):\n",
    "    list_words = cleanText(query) #cleaning query\n",
    "    res = set()\n",
    "    final_result = []\n",
    "\n",
    "    l=[set(sing_woq(word,index)) for word in list_words]\n",
    "    ids=list(set.intersection(*l)) \n",
    "        \n",
    "    #we use set because list does't recognize intersection as method\n",
    "    #ids=list(set.instersection(*l)) #gets id of documents containing all words in the query\n",
    "    \n",
    "    \n",
    "    #Check whether terms are in correct order\n",
    "    if len(ids) > 0: # If there is at least one document conatins all words in the query\n",
    "        \n",
    "        for id_doc in ids: # getting positions of words in documents\n",
    "            posMinusOne = []\n",
    "            list_pos=[index[word][id_doc][\"positions\"] for word in list_words] \n",
    "#             for word in list_words:\n",
    "#                 list_pos=[item[id_doc][\"positions\"] for item in index[word]] \n",
    "            for i, lpos in enumerate(list_pos) : #Check whether terms are in correct order\n",
    "                posMinusOne.append( set([x-i for x in lpos]) )\n",
    "                \n",
    "            if (len(set.intersection(*posMinusOne)) > 0):\n",
    "                final_result.append(id_doc)\n",
    "                \n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = phrase_query(list_text[1832], inv_index)\n",
    "query3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[inv_index[word][0][\"positions\"] for word in [\"september\",\"fall\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_text[1832]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index[\"china\"][97]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Autres façon de compter les fréquences des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def search_v3(query, index):\n",
    "    item_list = re.findall(\"[A-Za-z0-9]+\", query)\n",
    "    item_list2 = list(map(str.lower, item_list))\n",
    "    res = set()\n",
    "    final_Result = []\n",
    "    for i in range(len(item_list2)):\n",
    "        tmp = set(search_v1(item_list2[i], index))\n",
    "        if i ==0:\n",
    "            res = tmp.intersection(tmp)\n",
    "        else :\n",
    "            res = res.intersection(tmp)\n",
    "    \n",
    "    if len(res) > 0:\n",
    "        list_pos = []\n",
    "        isNear = []\n",
    "        for doc_num in res:\n",
    "            for item in item_list2:\n",
    "                list_pos.append(getPosition(item, doc_num, index))\n",
    "            min_occurence = min ([len(i) for i in list_pos])\n",
    "            firstDocPos = list_pos[0]\n",
    "            list_other_Docs = list_pos[1:]\n",
    "            for position in firstDocPos:\n",
    "                for other_Pos in list_other_Docs:\n",
    "                    for p in other_Pos:\n",
    "                        isNear.append(abs(p-position))\n",
    "            if (1 in isNear):\n",
    "                final_Result.append(doc_num)\n",
    "    return final_Result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">Ordered Queries</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orderd queries by absolute frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_v1_sorted(query, index):\n",
    "    query = query.lower()\n",
    "    \n",
    "    try :\n",
    "        tmp = list(index[query].items())\n",
    "        tmp = sorted( list(map(lambda x:(x[0], x[1][\"occurencies\"]), tmp)), key=lambda a:a[1], reverse=True)\n",
    "        return list(map(lambda x:x[0], tmp))\n",
    "    \n",
    "    except :\n",
    "        print(\"No result for specified key!!!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_v1_sorted(\"china\", newIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "liste_rep[1407], liste_rep[774], liste_rep[2342]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered queries by weighted frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def search_v1_weighted(query, index):\n",
    "    query = query.lower()\n",
    "    \n",
    "    try :\n",
    "        tmp = list(index[query].items())\n",
    "        tmp = sorted( list(map(lambda x:(x[0], x[1][\"occurencies\"]/x[1][\"doc_size\"]), tmp)),\n",
    "                     key=lambda a:a[1], reverse=True)\n",
    "        return list(map(lambda x:x[0], tmp))\n",
    "    \n",
    "    except :\n",
    "        print(\"No result for specified key!!!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_v1_weighted(\"china\", newIndex)\n",
    "#newIndex[\"china\"][871]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "liste_rep[2409], liste_rep[2449], liste_rep[871]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ordered queries weighted by tf_idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_v1_tf_idf(query, index):\n",
    "    query = query.lower()\n",
    "    \n",
    "    try :\n",
    "        tmp = list(index[query].items())\n",
    "        N = len(tmp)\n",
    "        D = len(index)\n",
    "        tmp = sorted( list(map(lambda x:(x[0], \n",
    "                                         (x[1][\"occurencies\"]/x[1][\"doc_size\"])*np.log(D/(1+N)),\n",
    "                                        ), tmp)),\n",
    "                     key=lambda a:a[1], reverse=True)\n",
    "        return list(map(lambda x:x[0], tmp))\n",
    "    \n",
    "    except :\n",
    "        print(\"No result for specified key!!!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_v1_tf_idf(\"economy\", newIndex) == search_v1_weighted(\"economy\", newIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red; text-align:center\">Ordering for free Text queries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_v2_sorted(query, index):\n",
    "    #item_list = re.findall(\"[a-z0-9]+\", query)\n",
    "    item_list = list(map(str.lower, re.findall(\"[a-z0-9]+\", query)))\n",
    "    res = []\n",
    "    for item in item_list:\n",
    "        tmp = search_v1(item, index)\n",
    "        if tmp:\n",
    "            res.extend(tmp)\n",
    "    return list(set(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newIndex\n",
    "#len(newIndex)\n",
    "len(list_vocab)\n",
    "#clean_Docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Bag of Words</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import Counter, OrderedDict\n",
    "Corpus = list_vocab\n",
    "v = DictVectorizer()\n",
    "\n",
    "v.fit([OrderedDict.fromkeys(Corpus, 1)])\n",
    "X = v.transform(Counter(f) for f in (clean_Docs))\n",
    "\n",
    "#print(type(X))\n",
    "#print(X.A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.where(X.A[0]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "docs = [[\"hello\", \"world\", \"hello\", \"alka\"], [\"goodbye\", \"cruel\", \"world\"]]\n",
    "indptr = [0]\n",
    "indices = []\n",
    "data = []\n",
    "vocabulary = {}\n",
    "for d in docs:\n",
    "    for term in d:\n",
    "        index = vocabulary.setdefault(term, len(vocabulary))\n",
    "        indices.append(index)\n",
    "        data.append(1)\n",
    "    indptr.append(len(indices))\n",
    "\n",
    "#csr_matrix((data, indices, indptr), dtype=int).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, data, indptr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
